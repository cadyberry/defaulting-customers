# Import test and training sets


library(psych)
library(caret)
library(ggplot2)
Set.seed(123) 

# Target variable: Default
# All four predictors are continuous.

# 1 = positive = default, no loan given, no action taken
# 0 = negative = no default, loan is given, action taken

# Step 1. Data Prep

# Set Default as categorical
ltest$Default<-as.factor(ltest$Default)
ltrain$Default<-as.factor(ltrain$Default)

# EDA

# Variable: Default
summary(ltrain$Default)
    # Default = 0 = 65203
    #         = 1 = 22417

# number of defaults in test
summary(ltest$Default)
    #     0     1
    #65086  22719 


# Summary statistics 

# use mean loan and mean interest amount for cost matrix
# loan amt calculated as:
# interest_amt=(installmentÃ—term)-loan_amnt.

# Explanatory variables
summary(ltrain$int_rate)
# mean = 13.46
summary(ltrain$loan_amnt)
# mean = 14514
summary(ltrain$installment)
# mean = 447.04
summary(ltrain$term)
# 36 - 67687
# 60 = 19933


# Variable: Interest Amount
ltrain$interest_amt <- ((ltrain$installment *ltrain$term) - ltrain$loan_amnt)
ltest$interest_amt <- ((ltest$installment *ltest$term) - ltest$loan_amnt)
# *Interest_amt is not a predictor*

summary(ltrain$interest_amt)
# 4391.66

# Cost Matrix
  #CostTN = $ 0
  #CostFP  = $ 1 profits
  #CostFN = $ 3.3 losses
  #CostTP = 0

# Scoring Matrix
  #CostTN = $ -4,392 profits
  #CostFP  = $ 0
  #CostFN = $ 14,514 losses
  #CostTP = $ 0


# Step 2: Establish Baseline Model Performance

# All positive model: 
# All defaulting customers, no loans given
# TN  = 0	FP = 65,086
# FN  = 0	TP = 22,719

# METRICS
# Accuracy = tn + tp / gt = 22719 / 87805 =  0.2587438
# Error rate = 1-accuracy
# Sensitivity = true pos/ total actually pos = 22719/ (22719) =  1
# Specificity = true neg/ total actually neg = 0 
# Precision = true pos / total predicted pos = 22719/ 87805 = 0.2587438
# Recall = sensitivity

# Fbeta scores
#F1
# 1 + 1^2 * (0.2587438 * 1) / ((1^2 *0.2587438 ) + 1) =  0.2211838
# 2 * 0.2211838
# F1 =  0.4111143
#F2
# 1 + beta^2* (0.2587438 *1)/ (((2^2) * 0.2587438) + 1)
# (1 + 2^2) *  0.1271484
# f2 = 0.6357419
#F3
# 1 + beta * (0.2587438 *1)/ (((0.5^0.5) * 0.2587438) + 1)
# (1 + 0.5^2) *  0.2187258
# f0.5 =1.25* = 0.2734073

# Scoring matrix to determine total model cost:
# calculated using interest amount 
#CostTN = $ -4,392
#CostFP  = $ 0
#CostFN = $ 14,514
#CostTP = $ 0

# Total model cost = (0 * -4392 ) + (65086*0) + (0*14514) + (22719*0) = 0
# calculate cost per customer = 0


# All negative model
# No defaults, all loans given
# TN = 65,086	FP = 0
# FN= 22,719 	TP = 0


              ## Metrics ##
# Accuracy = tn + tp / gt  = 65086/87805
# Error rate = opposite accuracy
# Sensitivity = true pos/ total actually pos = 0 
# Specificity = true neg/ total actually neg = 1
# Precision = true pos / total predicted pos = 0 
# Recall = sensitivity = 0 

Scoring matrix:
#CostTN = $ -4,392
#CostFP  = $ 0
#CostFN = $ 14,514
#CostTP = $ 0

# total model cost = (65086 * -4392 ) + (0*0) + (22719*14514) + (0*0) = 43,885,854
# Cost per customer = 43885854/87805 = 499.8104



# Model Building

# Set term as categorical: 36 months/ 64 months
ltest$term<-as.factor(ltest$term)
ltrain$term<-as.factor(ltrain$term)

# Use training set to develop CART model 
# Standardize data sets so larger variables don't overpower the others
# preprocess from caret

# Standardize train data set
preprocess.train.z <- preProcess(ltrain[1:5], method=c("center", "scale") )
preprocess.train.z

ltrain.z <-
  predict(preprocess.train.z, ltrain[1:5] )
View(ltrain.z)

# Standardize test data set
preprocess.test.z <- preProcess(ltest[1:5], method=c("center", "scale") )
preprocess.test.z

ltest.z <-
  predict(preprocess.test.z, ltest[1:5] )
View(ltest.z)

# Descriptive stats
describe(ltest.z)
describe(ltrain.z)

###################
# Naive  CART model
# CART 10 fold cross validation 
set.seed(107)
TC <- trainControl(
  method = "CV",
  number = 10)

fita<- train(Default ~ ., 
                    data = ltrain.z,
                    method = "rpart",
                    trControl = TC)

# Check for overfitting by analyzing metrics on folds
fita$resample


# Apply model to the standardized test data set
naive_model <- predict(fita, ltest.z)


# Contingency table of predictions vs actual default values
table(ltest.z$Default, naive_model)
#     0     1
#0 62579  2507
#1 20050  2669

                   ### Metrics ###
# accuracy = tn + tp / gt = 62579+2669 / 87805 =  0.7431012
# sensitivity = true pos/ total actually pos = 2669/ (2669+20050) =  0.1174
# specificity = true neg/ total actually neg = 62579/ (2507+62579) = 0.9614817 # performs much better with identifying true negatives
# precision = true pos / total predicted pos = 2669 / (2507 +2669) = 0.5156491
# recall = sensitivity

# FB scores
# (1+b^2)* 
# (precision * recall / 
# ((beta^2 * precision) + recall))

# 1 + 1^2 * (0.5156491*  0.1174) / (((1^2) *0.5156491 ) +  0.1174) =   0.09562798
# 2 * 0.09562798
# f1 =   0.191256

# 1 + beta^2* (0.5156491 * 0.1174)/ (((2^2) * 0.5156491) + 0.1174)
# (1 + 2^2) *  0.02776941
# f2 = 0.138847

# 1 + beta * (0.5156491 * 0.1174)/ (((0.5^0.5) * 0.5156491) +  0.1174)
# (1 + 0.5^2) *  0.1255909
# f0.5 =1.25* = 0.1569886


# Model cost
#TN= -4392 # from scoring matrix 
#FN = 14514

# Model Cost: ((62579*-4392) + (20050*14514))/ 87805=  184.0297


###################
# 3.3x CART model #

# resampling as a surrogate for misclassification costs
# 1 Re-sample records
# Get rare record indices with positive responses
rare.record.indices <- 
  which(ltrain.z$Default == 1)

# 2  Resample rare record indices
rare.indices.resampled <- 
  sample(x = rare.record.indices, 
         size = 51559, replace = TRUE)

# 3 Extract the resampled records
rare.records.resampled <- 
  ltrain.z[rare.indices.resampled,]

#  4 Merge extracted records with train set
set.seed(107)
ltrain.3.3x <- 
  rbind(ltrain.z, rare.records.resampled)

# table 
table(ltrain.3.3x$Default)
  #0     1 
  #65203 73976 # resampling successful 

# 3.3x CART
set.seed(107)
TC <- trainControl(method = "CV", number = 10)
fitb <- train(Default ~ ., 
                    data = ltrain.3.3x,
                    method = "rpart",
                    trControl = TC)


# Check for overfitting
fitb$resample


# Apply model to the test set
resampled_model <- predict(fitb, ltest.z)


# contingency table of predict vs actual Default values
table(ltest.z$Default, resampled_model)
#          Predicted
#           0     1
# Actual 0 34429 30657
#       1  6050 16669
# reduces losses

# variable: default
summary(ltrain.3.3x$Default)
# total negative 65203 
# total positive in resampled = 73976 
# total in resampled =   139179



                   ### METRICS ###
# accuracy = tn + tp / gt = (16650+34478) / 139179 =  0.3673543
# sensitivity = true pos/ total actually pos = 16669/(30657+16669) =   0.3522165
# specificity = true neg/ total actually neg = 34429/ (34429+30657) = 0.5297299

# precision = true pos / total predicted pos = 16650 / (16650 +30657) = 0.3519564
# recall = sensitivity

# FB scores
# (1+b^2)* 
# (precision * recall / 
# ((beta^2 * precision) + recall))

#F1
# 1 + 1^2 * (0.3519564*  0.5297299) / (((1^2) *0.3519564 ) +  0.5297299) =   0.09562798
# 2 *  0.2114605
# F1 =   0.422921
#F2
# 1 + beta^2* (0.3519564 * 0.5297299)/ (((2^2) * 0.3519564) + 0.5297299)
# 5 *  0.09622528
# f2 =  0.4811264
#F.5
# 1 + beta * (0.3519564 * 0.5297299)/ (((0.5^0.5) * 0.3519564) +  0.5297299)
# (1 + 0.5^2) *  0.2394576
# 1.25*0.1255909 = 0.299322


#model cost
#TN= -4392 #scoring matrix
#FN = 14514

# COST: ((34429*-4392) + (6050*14514)) / 87805= -722.08

# 3.3x Decision tree

library(rattle)
fancyRpartPlot(fitb$finalModel)

# un-standardization of vars for decision rule interpretation 
# y = Z-score * sd + mean

# int rate 
(-0.33 * 5.16) + 13.46 = 11.7572
(0.15 * 5.16) +  13.46 = 14.234
(0.71 * 5.16) +  13.46 = 17.1236

#installment
(-1 *276.35 + 447.04) = 170.69


